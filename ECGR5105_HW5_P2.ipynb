{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP/npi6ZYTtBW7NUtVH8Duh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Braelin2/ECGR5105HW5/blob/main/ECGR5105_HW5_P2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYV75zUqG7A6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch as torch\n",
        "import tensorflow as tf\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/My Drive/ECGR5105/Housing.csv'\n",
        "dataset = pd.DataFrame(pd.read_csv(file_path))"
      ],
      "metadata": {
        "id": "hRaw-CoHHNwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model(t_u, w5, w4, w3, w2, w1, b):\n",
        "    return (w5*t_u**5) + (w4*t_u**4) + (w3*t_u**3) + (w2*t_u**2) + (w1*t_u) + b"
      ],
      "metadata": {
        "id": "LxGzfy0PHU09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(t_p, t_c):\n",
        "  squared_diffs = (t_p - t_c)**2\n",
        "  return squared_diffs.mean()"
      ],
      "metadata": {
        "id": "N_IsSEvJHsrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop(n_epochs, optimizer, params, t_u_train, t_u_test, t_c_train, t_c_test):\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    if params.grad is not None:\n",
        "      params.grad.zero_()\n",
        "\n",
        "    t_p_train = model(t_u_train, *params)\n",
        "    train_loss = loss_fn(t_p_train.transpose(0,1) , t_c_train)\n",
        "\n",
        "    t_p_test = model(t_u_test, *params)\n",
        "    test_loss = loss_fn(t_p_test.transpose(0,1), t_c_test)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    train_loss.backward()\n",
        "    test_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 500 == 0:\n",
        "     print('Epoch %d, Training Loss %d, Testing Loss %f' % (epoch, float(train_loss), float(test_loss)))\n",
        "  return params"
      ],
      "metadata": {
        "id": "xPq3aWw6H0sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "np.random.seed(0)\n",
        "training, testing = train_test_split(dataset, train_size = 0.8, test_size = 0.2, random_state = 100)\n",
        "\n",
        "Y_train = training.pop(\"price\")\n",
        "Y_test = testing.pop(\"price\")\n",
        "\n",
        "varlist =  ['area', 'bedrooms', 'bathrooms', 'stories', 'parking']"
      ],
      "metadata": {
        "id": "-ibTk1K3yNtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "standScaler = StandardScaler()\n",
        "\n",
        "X_train_temp = training\n",
        "X_train_temp[varlist] = standScaler.fit_transform(training[varlist])\n",
        "\n",
        "X_test_temp = testing\n",
        "X_test_temp[varlist] = standScaler.fit_transform(testing[varlist])\n",
        "\n",
        "num_train = len(X_train_temp)\n",
        "num_test = len(X_test_temp)\n",
        "\n",
        "X_train_temp1 = np.c_[np.ones((num_train, 1)), X_train_temp[varlist]]\n",
        "X_test_temp1 = np.c_[np.ones((num_test, 1)), X_test_temp[varlist]]\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train_temp1)\n",
        "X_test_tensor = torch.tensor(X_test_temp1)\n",
        "\n",
        "Y_train_tensor = torch.tensor(Y_train.values)\n",
        "Y_test_tensor = torch.tensor(Y_test.values)\n",
        "\n",
        "Y_train_normal = (Y_train_tensor - Y_train_tensor.float().mean())/Y_train.std()\n",
        "Y_test_normal = (Y_test_tensor - Y_test_tensor.float().mean())/Y_test.std()"
      ],
      "metadata": {
        "id": "MsyO0nFwKOcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SGD Optimizer:\n",
        "# Learning Rate 0.00001\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad = True)\n",
        "learning_rate = 1e-5\n",
        "optimizer = optim.SGD([params], lr = learning_rate)\n",
        "\n",
        "training_loop(n_epochs = 5000, optimizer = optimizer, params = params, t_u_train = X_train_tensor, t_u_test = X_test_tensor, t_c_train = Y_train_tensor, t_c_test = Y_test_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMwGSs4aKp9H",
        "outputId": "1d2e35b5-234c-4817-d6f1-bf148cbc3981"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 500, Training Loss 22809144445701, Testing Loss 21404015541633.937500\n",
            "Epoch 1000, Training Loss 21378466024792, Testing Loss 19890195314928.925781\n",
            "Epoch 1500, Training Loss 20435487381935, Testing Loss 18903799955535.957031\n",
            "Epoch 2000, Training Loss 19631986636813, Testing Loss 18102701316564.980469\n",
            "Epoch 2500, Training Loss 18883526321979, Testing Loss 17385131306782.707031\n",
            "Epoch 3000, Training Loss 18171079428626, Testing Loss 16718094835498.162109\n",
            "Epoch 3500, Training Loss 17490686522498, Testing Loss 16089323554483.707031\n",
            "Epoch 4000, Training Loss 16841269899588, Testing Loss 15493303723187.179688\n",
            "Epoch 4500, Training Loss 16221986124445, Testing Loss 14926956409258.468750\n",
            "Epoch 5000, Training Loss 15631819485096, Testing Loss 14388197279743.562500\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-34429.5430, 138333.7656,  89859.0547, 458742.9375, 102777.7422,\n",
              "        745468.9375], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SGD Optimizer:\n",
        "# Learning Rate 0.000001\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad = True)\n",
        "learning_rate = 1e-6\n",
        "optimizer = optim.SGD([params], lr = learning_rate)\n",
        "\n",
        "training_loop(n_epochs = 5000, optimizer = optimizer, params = params, t_u_train = X_train_tensor, t_u_test = X_test_tensor, t_c_train = Y_train_tensor, t_c_test = Y_test_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4bR6Y3xgQK_",
        "outputId": "91713129-65c1-46c3-cba7-efd9627a0001"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 500, Training Loss 25588593254810, Testing Loss 24164663383531.261719\n",
            "Epoch 1000, Training Loss 25136998359850, Testing Loss 23730594860869.046875\n",
            "Epoch 1500, Training Loss 24732587618588, Testing Loss 23337909024454.121094\n",
            "Epoch 2000, Training Loss 24369405956576, Testing Loss 22981677805386.851562\n",
            "Epoch 2500, Training Loss 24042251453523, Testing Loss 22657577656720.839844\n",
            "Epoch 3000, Training Loss 23746574357713, Testing Loss 22361808779132.839844\n",
            "Epoch 3500, Training Loss 23478398771497, Testing Loss 22091033947162.433594\n",
            "Epoch 4000, Training Loss 23234249662684, Testing Loss 21842320555873.164062\n",
            "Epoch 4500, Training Loss 23011091247504, Testing Loss 21613091855962.484375\n",
            "Epoch 5000, Training Loss 22806269587144, Testing Loss 21401081546176.207031\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-22909.2617, 115913.3359,  59107.9883,  77523.8203,  21004.1074,\n",
              "         89170.0078], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SGD Optimizer:\n",
        "# Learning Rate 0.0000001\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad = True)\n",
        "learning_rate = 1e-7\n",
        "optimizer = optim.SGD([params], lr = learning_rate)\n",
        "\n",
        "training_loop(n_epochs = 5000, optimizer = optimizer, params = params, t_u_train = X_train_tensor, t_u_test = X_test_tensor, t_c_train = Y_train_tensor, t_c_test = Y_test_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03TygJpSgQuP",
        "outputId": "a6d33822-b106-4137-e92c-61d9dcbcb933"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 500, Training Loss 26029840326639, Testing Loss 24609494407248.542969\n",
            "Epoch 1000, Training Loss 25986515783321, Testing Loss 24545527239121.125000\n",
            "Epoch 1500, Training Loss 25935400673159, Testing Loss 24495298385906.613281\n",
            "Epoch 2000, Training Loss 25884076314267, Testing Loss 24446409789116.261719\n",
            "Epoch 2500, Training Loss 25833277775813, Testing Loss 24398086248521.886719\n",
            "Epoch 3000, Training Loss 25783051494813, Testing Loss 24350266621197.656250\n",
            "Epoch 3500, Training Loss 25733393731665, Testing Loss 24302940986485.992188\n",
            "Epoch 4000, Training Loss 25684297059035, Testing Loss 24256102709461.597656\n",
            "Epoch 4500, Training Loss 25635754113291, Testing Loss 24209745737585.027344\n",
            "Epoch 5000, Training Loss 25587757195527, Testing Loss 24163863681118.867188\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 1857.8185, 16858.9121,  8209.5625,  8944.8672,  2526.6177,  9280.1904],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SGD Optimizer:\n",
        "# Learning Rate 0.000000001\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad = True)\n",
        "learning_rate = 1e-8\n",
        "optimizer = optim.SGD([params], lr = learning_rate)\n",
        "\n",
        "training_loop(n_epochs = 5000, optimizer = optimizer, params = params, t_u_train = X_train_tensor, t_u_test = X_test_tensor, t_c_train = Y_train_tensor, t_c_test = Y_test_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hB3ZAjCngRbu",
        "outputId": "eae0f38d-73c0-4ec2-f7c2-48f1c76c3c29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 500, Training Loss 26283880008515, Testing Loss 24977338426172.429688\n",
            "Epoch 1000, Training Loss 26174017064096, Testing Loss 24859944198067.160156\n",
            "Epoch 1500, Training Loss 26112375866292, Testing Loss 24782442210813.625000\n",
            "Epoch 2000, Training Loss 26078037364895, Testing Loss 24729959506744.035156\n",
            "Epoch 2500, Training Loss 26058888524340, Testing Loss 24693356537177.320312\n",
            "Epoch 3000, Training Loss 26047974447300, Testing Loss 24666970282847.609375\n",
            "Epoch 3500, Training Loss 26041347808254, Testing Loss 24647256653603.339844\n",
            "Epoch 4000, Training Loss 26036804694554, Testing Loss 24631971293539.429688\n",
            "Epoch 4500, Training Loss 26033144177752, Testing Loss 24619674340685.718750\n",
            "Epoch 5000, Training Loss 26029735860167, Testing Loss 24609427952609.796875\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([4909.7744, 2759.7720, 1077.9115,  963.7599,  272.8635,  936.1778],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ADAM Optimizer\n",
        "# Learning Rate 0.1\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad = True)\n",
        "learning_rate = 1e-1\n",
        "optimizer = optim.Adam([params], lr = learning_rate)\n",
        "\n",
        "training_loop(n_epochs = 5000, optimizer = optimizer, params = params, t_u_train = X_train_tensor, t_u_test = X_test_tensor, t_c_train = Y_train_tensor, t_c_test = Y_test_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWv42YBPgU-d",
        "outputId": "c7242b40-5635-4499-c578-1d0bebf3354c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 500, Training Loss 26465369593497, Testing Loss 25147493382079.199219\n",
            "Epoch 1000, Training Loss 26453517368410, Testing Loss 25136531363643.414062\n",
            "Epoch 1500, Training Loss 26441825489182, Testing Loss 25125679166617.507812\n",
            "Epoch 2000, Training Loss 26430285712800, Testing Loss 25114929634774.218750\n",
            "Epoch 2500, Training Loss 26418891068480, Testing Loss 25104276675231.632812\n",
            "Epoch 3000, Training Loss 26407635913034, Testing Loss 25093715335461.871094\n",
            "Epoch 3500, Training Loss 26396515669544, Testing Loss 25083241567224.332031\n",
            "Epoch 4000, Training Loss 26385527082944, Testing Loss 25072852499594.910156\n",
            "Epoch 4500, Training Loss 26374667308277, Testing Loss 25062545598808.515625\n",
            "Epoch 5000, Training Loss 26363934604568, Testing Loss 25052319317649.429688\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([493.3556, 496.0881, 497.5134, 499.7987, 499.8281, 499.8968],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ADAM Optimizer\n",
        "# Learning Rate 0.001\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad = True)\n",
        "learning_rate = 1e-3\n",
        "optimizer = optim.Adam([params], lr = learning_rate)\n",
        "\n",
        "training_loop(n_epochs = 5000, optimizer = optimizer, params = params, t_u_train = X_train_tensor, t_u_test = X_test_tensor, t_c_train = Y_train_tensor, t_c_test = Y_test_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6jwJU4wgVsN",
        "outputId": "580c7288-e36d-4849-c2ce-823ebe54b30e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 500, Training Loss 26477246390584, Testing Loss 25158439755920.660156\n",
            "Epoch 1000, Training Loss 26477125340148, Testing Loss 25158328378735.695312\n",
            "Epoch 1500, Training Loss 26477004302495, Testing Loss 25158217009544.128906\n",
            "Epoch 2000, Training Loss 26476883277448, Testing Loss 25158105648158.726562\n",
            "Epoch 2500, Training Loss 26476762265577, Testing Loss 25157994295133.332031\n",
            "Epoch 3000, Training Loss 26476641266314, Testing Loss 25157882949915.765625\n",
            "Epoch 3500, Training Loss 26476520279694, Testing Loss 25157771612536.527344\n",
            "Epoch 4000, Training Loss 26476399305644, Testing Loss 25157660282932.386719\n",
            "Epoch 4500, Training Loss 26476278344198, Testing Loss 25157548961132.738281\n",
            "Epoch 5000, Training Loss 26476157395360, Testing Loss 25157437647142.027344\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([5.9997, 5.9997, 5.9997, 5.9997, 5.9997, 4.9998], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ADAM Optimizer\n",
        "# Learning Rate 0.0001\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad = True)\n",
        "learning_rate = 1e-4\n",
        "optimizer = optim.Adam([params], lr = learning_rate)\n",
        "\n",
        "training_loop(n_epochs = 5000, optimizer = optimizer, params = params, t_u_train = X_train_tensor, t_u_test = X_test_tensor, t_c_train = Y_train_tensor, t_c_test = Y_test_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEaNAl4UgWR9",
        "outputId": "b55693c4-1030-46cb-8ad6-d25d619497de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 500, Training Loss 26477355130590, Testing Loss 25158539803249.910156\n",
            "Epoch 1000, Training Loss 26477343021111, Testing Loss 25158528661970.796875\n",
            "Epoch 1500, Training Loss 26477330911753, Testing Loss 25158517520765.394531\n",
            "Epoch 2000, Training Loss 26477318802522, Testing Loss 25158506379638.089844\n",
            "Epoch 2500, Training Loss 26477306693417, Testing Loss 25158495238588.894531\n",
            "Epoch 3000, Training Loss 26477294584509, Testing Loss 25158484097687.105469\n",
            "Epoch 3500, Training Loss 26477282475727, Testing Loss 25158472956863.414062\n",
            "Epoch 4000, Training Loss 26477270367072, Testing Loss 25158461816117.828125\n",
            "Epoch 4500, Training Loss 26477258258543, Testing Loss 25158450675450.351562\n",
            "Epoch 5000, Training Loss 26477246150140, Testing Loss 25158439534860.980469\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.5001, 1.5001, 1.5001, 1.5001, 1.5001, 0.5000], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ADAM Optimizer\n",
        "# Learning Rate 0.00001\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad = True)\n",
        "learning_rate = 1e-5\n",
        "optimizer = optim.Adam([params], lr = learning_rate)\n",
        "\n",
        "training_loop(n_epochs = 5000, optimizer = optimizer, params = params, t_u_train = X_train_tensor, t_u_test = X_test_tensor, t_c_train = Y_train_tensor, t_c_test = Y_test_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UPDO9KKgXBo",
        "outputId": "7755b8ac-4551-41a2-d1b9-c2ad5afcd166"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 500, Training Loss 26477366006041, Testing Loss 25158549809136.164062\n",
            "Epoch 1000, Training Loss 26477364793691, Testing Loss 25158548693726.160156\n",
            "Epoch 1500, Training Loss 26477363581343, Testing Loss 25158547578318.160156\n",
            "Epoch 2000, Training Loss 26477362368993, Testing Loss 25158546462907.160156\n",
            "Epoch 2500, Training Loss 26477361156643, Testing Loss 25158545347496.390625\n",
            "Epoch 3000, Training Loss 26477359944295, Testing Loss 25158544232086.406250\n",
            "Epoch 3500, Training Loss 26477358731955, Testing Loss 25158543116683.707031\n",
            "Epoch 4000, Training Loss 26477357519618, Testing Loss 25158542001283.949219\n",
            "Epoch 4500, Training Loss 26477356307282, Testing Loss 25158540885884.972656\n",
            "Epoch 5000, Training Loss 26477355094948, Testing Loss 25158539770486.781250\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.0501, 1.0501, 1.0501, 1.0501, 1.0501, 0.0500], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    }
  ]
}