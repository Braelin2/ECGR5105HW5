{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPguCFDqy4cAisHBGAp69gI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Braelin2/ECGR5105HW5/blob/main/ECGR5105_HW5_P3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYV75zUqG7A6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch as torch\n",
        "import tensorflow as tf\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/My Drive/ECGR5105/Housing.csv'\n",
        "dataset = pd.DataFrame(pd.read_csv(file_path))"
      ],
      "metadata": {
        "id": "hRaw-CoHHNwY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Applying a binary map\n",
        "convlist =  ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea', 'furnishingstatus']\n",
        "\n",
        "# Defining the map function\n",
        "def Convert(x):\n",
        "    return x.map({'yes': 1, 'no': 0, 'furnished':  1, 'semi-furnished':  0, 'unfurnished':  -1})\n",
        "\n",
        "dataset[convlist] = dataset[convlist].apply(Convert)"
      ],
      "metadata": {
        "id": "uP7oI2B4ieqJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model(t_u, w5, w4, w3, w2, w1, b):\n",
        "    return (w5*t_u**5) + (w4*t_u**4) + (w3*t_u**3) + (w2*t_u**2) + (w1*t_u) + b"
      ],
      "metadata": {
        "id": "LxGzfy0PHU09"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(t_p, t_c):\n",
        "  squared_diffs = (t_p - t_c)**2\n",
        "  return squared_diffs.mean()"
      ],
      "metadata": {
        "id": "N_IsSEvJHsrh"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop(n_epochs, optimizer, params, t_u_train, t_u_test, t_c_train, t_c_test):\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    if params.grad is not None:\n",
        "      params.grad.zero_()\n",
        "\n",
        "    t_p_train = model(t_u_train, *params)\n",
        "    train_loss = loss_fn(t_p_train.transpose(0,1) , t_c_train)\n",
        "\n",
        "    t_p_test = model(t_u_test, *params)\n",
        "    test_loss = loss_fn(t_p_test.transpose(0,1), t_c_test)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    train_loss.backward()\n",
        "    test_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 500 == 0:\n",
        "     print('Epoch %d, Training Loss %d, Testing Loss %f' % (epoch, float(train_loss), float(test_loss)))\n",
        "  return params"
      ],
      "metadata": {
        "id": "xPq3aWw6H0sr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "np.random.seed(0)\n",
        "training, testing = train_test_split(dataset, train_size = 0.8, test_size = 0.2, random_state = 100)\n",
        "\n",
        "Y_train = training.pop(\"price\")\n",
        "Y_test = testing.pop(\"price\")\n",
        "\n",
        "varlist =  ['area', 'bedrooms', 'bathrooms', 'stories', 'mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'parking', 'prefarea', 'furnishingstatus']"
      ],
      "metadata": {
        "id": "-ibTk1K3yNtM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "standScaler = StandardScaler()\n",
        "\n",
        "X_train_temp = training\n",
        "X_train_temp[varlist] = standScaler.fit_transform(training[varlist])\n",
        "\n",
        "X_test_temp = testing\n",
        "X_test_temp[varlist] = standScaler.fit_transform(testing[varlist])\n",
        "\n",
        "num_train = len(X_train_temp)\n",
        "num_test = len(X_test_temp)\n",
        "\n",
        "X_train_temp1 = np.c_[np.ones((num_train, 1)), X_train_temp[varlist]]\n",
        "X_test_temp1 = np.c_[np.ones((num_test, 1)), X_test_temp[varlist]]\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train_temp1)\n",
        "X_test_tensor = torch.tensor(X_test_temp1)\n",
        "\n",
        "Y_train_tensor = torch.tensor(Y_train.values)\n",
        "Y_test_tensor = torch.tensor(Y_test.values)\n",
        "\n",
        "Y_train_normal = (Y_train_tensor - Y_train_tensor.float().mean())/Y_train.std()\n",
        "Y_test_normal = (Y_test_tensor - Y_test_tensor.float().mean())/Y_test.std()"
      ],
      "metadata": {
        "id": "MsyO0nFwKOcb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SGD Optimizer:\n",
        "# Learning Rate 0.00000001\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad = True)\n",
        "learning_rate = 1e-8\n",
        "optimizer = optim.SGD([params], lr = learning_rate)\n",
        "\n",
        "training_loop(n_epochs = 5000, optimizer = optimizer, params = params, t_u_train = X_train_tensor, t_u_test = X_test_tensor, t_c_train = Y_train_tensor, t_c_test = Y_test_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMwGSs4aKp9H",
        "outputId": "71aae6d8-8975-4c46-b0b1-de5be1c2c733"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 500, Training Loss 26459096035097, Testing Loss 25143264865888.750000\n",
            "Epoch 1000, Training Loss 26444845052613, Testing Loss 25132358368028.679688\n",
            "Epoch 1500, Training Loss 26430710470895, Testing Loss 25121518152440.433594\n",
            "Epoch 2000, Training Loss 26416691335785, Testing Loss 25110743763004.058594\n",
            "Epoch 2500, Training Loss 26402786597260, Testing Loss 25100034669347.574219\n",
            "Epoch 3000, Training Loss 26388995417359, Testing Loss 25089390491732.222656\n",
            "Epoch 3500, Training Loss 26375316710634, Testing Loss 25078810637348.027344\n",
            "Epoch 4000, Training Loss 26361749520358, Testing Loss 25068294669647.847656\n",
            "Epoch 4500, Training Loss 26348293052148, Testing Loss 25057842211578.296875\n",
            "Epoch 5000, Training Loss 26334946275589, Testing Loss 25047452741661.707031\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-274.0860, 3027.1482, 1072.3868,  979.4740,  191.0082,  943.2203],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SGD Optimizer:\n",
        "# Learning Rate 0.00000001\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad = True)\n",
        "learning_rate = 1e-9\n",
        "optimizer = optim.SGD([params], lr = learning_rate)\n",
        "\n",
        "training_loop(n_epochs = 5000, optimizer = optimizer, params = params, t_u_train = X_train_tensor, t_u_test = X_test_tensor, t_c_train = Y_train_tensor, t_c_test = Y_test_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4bR6Y3xgQK_",
        "outputId": "14567145-11ba-4045-bc47-5d3e7e41b30c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 500, Training Loss 26471996416988, Testing Loss 25153117893571.453125\n",
            "Epoch 1000, Training Loss 26470555370290, Testing Loss 25152018340510.398438\n",
            "Epoch 1500, Training Loss 26469115611994, Testing Loss 25150919342884.179688\n",
            "Epoch 2000, Training Loss 26467677036136, Testing Loss 25149821031827.574219\n",
            "Epoch 2500, Training Loss 26466239628482, Testing Loss 25148723369816.101562\n",
            "Epoch 3000, Training Loss 26464803404318, Testing Loss 25147626397670.937500\n",
            "Epoch 3500, Training Loss 26463368383026, Testing Loss 25146530105929.167969\n",
            "Epoch 4000, Training Loss 26461934517660, Testing Loss 25145434464764.566406\n",
            "Epoch 4500, Training Loss 26460501891860, Testing Loss 25144339553108.066406\n",
            "Epoch 5000, Training Loss 26459070391654, Testing Loss 25143245257171.273438\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ -2.3934, 311.7432, 110.0760,  99.5430,  20.1588,  94.4241],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SGD Optimizer:\n",
        "# Learning Rate 0.0000000001\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad = True)\n",
        "learning_rate = 1e-10\n",
        "optimizer = optim.SGD([params], lr = learning_rate)\n",
        "\n",
        "training_loop(n_epochs = 5000, optimizer = optimizer, params = params, t_u_train = X_train_tensor, t_u_test = X_test_tensor, t_c_train = Y_train_tensor, t_c_test = Y_test_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03TygJpSgQuP",
        "outputId": "8a383e62-8f49-4af7-83e9-00f3666be5b9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 500, Training Loss 26474674450914, Testing Loss 25153703364291.375000\n",
            "Epoch 1000, Training Loss 26473631102372, Testing Loss 25153632547562.726562\n",
            "Epoch 1500, Training Loss 26473172393810, Testing Loss 25153731581153.527344\n",
            "Epoch 2000, Training Loss 26472918111244, Testing Loss 25153718674942.488281\n",
            "Epoch 2500, Training Loss 26472735376785, Testing Loss 25153645571912.632812\n",
            "Epoch 3000, Training Loss 26472577689264, Testing Loss 25153548830678.539062\n",
            "Epoch 3500, Training Loss 26472428775992, Testing Loss 25153443503094.031250\n",
            "Epoch 4000, Training Loss 26472282945351, Testing Loss 25153335136538.996094\n",
            "Epoch 4500, Training Loss 26472138188453, Testing Loss 25153225705167.769531\n",
            "Epoch 5000, Training Loss 26471993858502, Testing Loss 25153115902242.468750\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([25.3401, 34.5134, 12.1553, 10.8830,  2.9196,  9.4436],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SGD Optimizer:\n",
        "# Learning Rate 0.00000000001\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad = True)\n",
        "learning_rate = 1e-11\n",
        "optimizer = optim.SGD([params], lr = learning_rate)\n",
        "\n",
        "training_loop(n_epochs = 5000, optimizer = optimizer, params = params, t_u_train = X_train_tensor, t_u_test = X_test_tensor, t_c_train = Y_train_tensor, t_c_test = Y_test_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hB3ZAjCngRbu",
        "outputId": "fdedbe16-4f68-4d82-9772-d4d68d8a43ab"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 500, Training Loss 26476976950461, Testing Loss 25157107992377.652344\n",
            "Epoch 1000, Training Loss 26476607736151, Testing Loss 25156229878219.242188\n",
            "Epoch 1500, Training Loss 26476273991027, Testing Loss 25155547459539.578125\n",
            "Epoch 2000, Training Loss 26475972157173, Testing Loss 25155020565141.949219\n",
            "Epoch 2500, Training Loss 26475699034191, Testing Loss 25154616933177.101562\n",
            "Epoch 3000, Training Loss 26475451745945, Testing Loss 25154310689842.507812\n",
            "Epoch 3500, Training Loss 26475227704546, Testing Loss 25154081108810.621094\n",
            "Epoch 4000, Training Loss 26475024584609, Testing Loss 25153911616059.699219\n",
            "Epoch 4500, Training Loss 26474840291624, Testing Loss 25153788977010.265625\n",
            "Epoch 5000, Training Loss 26474672945494, Testing Loss 25153702649276.753906\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18.5082,  5.8057,  2.2565,  2.0022,  1.1934,  0.9445],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ADAM Optimizer\n",
        "# Learning Rate 0.1\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad = True)\n",
        "learning_rate = 1e-1\n",
        "optimizer = optim.Adam([params], lr = learning_rate)\n",
        "\n",
        "training_loop(n_epochs = 5000, optimizer = optimizer, params = params, t_u_train = X_train_tensor, t_u_test = X_test_tensor, t_c_train = Y_train_tensor, t_c_test = Y_test_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWv42YBPgU-d",
        "outputId": "e108255b-0174-4c55-cf4f-09dc10bde38b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 500, Training Loss 26470378502478, Testing Loss 25151671369199.375000\n",
            "Epoch 1000, Training Loss 26467117940590, Testing Loss 25149174476759.621094\n",
            "Epoch 1500, Training Loss 26463801491843, Testing Loss 25146443157525.039062\n",
            "Epoch 2000, Training Loss 26460402078713, Testing Loss 25143665149834.132812\n",
            "Epoch 2500, Training Loss 26456961927401, Testing Loss 25140865167504.164062\n",
            "Epoch 3000, Training Loss 26453501183149, Testing Loss 25138054996586.773438\n",
            "Epoch 3500, Training Loss 26450030129848, Testing Loss 25135240441886.683594\n",
            "Epoch 4000, Training Loss 26446554535130, Testing Loss 25132424670188.167969\n",
            "Epoch 4500, Training Loss 26443077672858, Testing Loss 25129609345654.093750\n",
            "Epoch 5000, Training Loss 26439601182137, Testing Loss 25126795151518.476562\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-22.5051, 476.6408, 493.5866, 500.0357, 500.2957, 499.9173],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ADAM Optimizer\n",
        "# Learning Rate 0.001\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad = True)\n",
        "learning_rate = 1e-3\n",
        "optimizer = optim.Adam([params], lr = learning_rate)\n",
        "\n",
        "training_loop(n_epochs = 5000, optimizer = optimizer, params = params, t_u_train = X_train_tensor, t_u_test = X_test_tensor, t_c_train = Y_train_tensor, t_c_test = Y_test_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6jwJU4wgVsN",
        "outputId": "47ca1140-52b3-4365-853c-ee8ee93aa957"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 500, Training Loss 26477273012280, Testing Loss 25157968053471.960938\n",
            "Epoch 1000, Training Loss 26477161782936, Testing Loss 25157714505614.828125\n",
            "Epoch 1500, Training Loss 26477051152597, Testing Loss 25157468679913.859375\n",
            "Epoch 2000, Training Loss 26476941012699, Testing Loss 25157230179810.855469\n",
            "Epoch 2500, Training Loss 26476831271950, Testing Loss 25156998679274.203125\n",
            "Epoch 3000, Training Loss 26476721852292, Testing Loss 25156773912925.414062\n",
            "Epoch 3500, Training Loss 26476612693847, Testing Loss 25156555675615.007812\n",
            "Epoch 4000, Training Loss 26476503748007, Testing Loss 25156343806826.273438\n",
            "Epoch 4500, Training Loss 26476394979663, Testing Loss 25156138189234.347656\n",
            "Epoch 5000, Training Loss 26476286364569, Testing Loss 25155938733836.046875\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([5.8534, 5.9353, 5.9737, 5.9964, 5.9981, 4.9998], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ADAM Optimizer\n",
        "# Learning Rate 0.0001\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad = True)\n",
        "learning_rate = 1e-4\n",
        "optimizer = optim.Adam([params], lr = learning_rate)\n",
        "\n",
        "training_loop(n_epochs = 5000, optimizer = optimizer, params = params, t_u_train = X_train_tensor, t_u_test = X_test_tensor, t_c_train = Y_train_tensor, t_c_test = Y_test_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEaNAl4UgWR9",
        "outputId": "94a343b5-d4cc-4ffd-9029-a328c01660a2"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 500, Training Loss 26477373528716, Testing Loss 25158202755189.843750\n",
            "Epoch 1000, Training Loss 26477362306788, Testing Loss 25158176287667.140625\n",
            "Epoch 1500, Training Loss 26477351093105, Testing Loss 25158149907952.066406\n",
            "Epoch 2000, Training Loss 26477339880226, Testing Loss 25158123592166.515625\n",
            "Epoch 2500, Training Loss 26477328674542, Testing Loss 25158097361147.878906\n",
            "Epoch 3000, Training Loss 26477317470273, Testing Loss 25158071197055.023438\n",
            "Epoch 3500, Training Loss 26477306268003, Testing Loss 25158045098612.960938\n",
            "Epoch 4000, Training Loss 26477295066916, Testing Loss 25158019064710.785156\n",
            "Epoch 4500, Training Loss 26477283865942, Testing Loss 25157993093903.175781\n",
            "Epoch 5000, Training Loss 26477272665082, Testing Loss 25157967186190.121094\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.4987, 1.4993, 1.4997, 1.5001, 1.5001, 0.5000], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ADAM Optimizer\n",
        "# Learning Rate 0.00001\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad = True)\n",
        "learning_rate = 1e-5\n",
        "optimizer = optim.Adam([params], lr = learning_rate)\n",
        "\n",
        "training_loop(n_epochs = 5000, optimizer = optimizer, params = params, t_u_train = X_train_tensor, t_u_test = X_test_tensor, t_c_train = Y_train_tensor, t_c_test = Y_test_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UPDO9KKgXBo",
        "outputId": "78cd7343-8439-4911-dcc5-9d1004958927"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 500, Training Loss 26477383613144, Testing Loss 25158226593564.648438\n",
            "Epoch 1000, Training Loss 26477382488499, Testing Loss 25158223932082.464844\n",
            "Epoch 1500, Training Loss 26477381363856, Testing Loss 25158221271238.328125\n",
            "Epoch 2000, Training Loss 26477380239210, Testing Loss 25158218611027.253906\n",
            "Epoch 2500, Training Loss 26477379114565, Testing Loss 25158215951452.464844\n",
            "Epoch 3000, Training Loss 26477377989921, Testing Loss 25158213292514.507812\n",
            "Epoch 3500, Training Loss 26477376865284, Testing Loss 25158210634219.882812\n",
            "Epoch 4000, Training Loss 26477375740652, Testing Loss 25158207976564.257812\n",
            "Epoch 4500, Training Loss 26477374616020, Testing Loss 25158205319545.468750\n",
            "Epoch 5000, Training Loss 26477373491389, Testing Loss 25158202663163.515625\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.0501, 1.0501, 1.0501, 1.0501, 1.0501, 0.0500], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    }
  ]
}